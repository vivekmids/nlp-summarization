{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To-Do: \n",
    "- Switch to 100d pretrained embeddings (reduce parameters) \n",
    "- Decoder layers to incorporate 4 hidden layers\n",
    "- Optimization: Adam, RMSProp? What's the difference? \n",
    "- Incorporate translation of decoder at the very end \n",
    "- How to incorporate ROUGE metric into accuracy \n",
    "- Split into train/validation/test such that we are consistent across the board when testing different models \n",
    "- research: how to save the models and keep them such that their training history + loss can be evaluated? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tensorflow_1_14/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/anaconda3/envs/tensorflow_1_14/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/anaconda3/envs/tensorflow_1_14/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/anaconda3/envs/tensorflow_1_14/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/anaconda3/envs/tensorflow_1_14/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/anaconda3/envs/tensorflow_1_14/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/anaconda3/envs/tensorflow_1_14/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/anaconda3/envs/tensorflow_1_14/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/anaconda3/envs/tensorflow_1_14/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/anaconda3/envs/tensorflow_1_14/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/anaconda3/envs/tensorflow_1_14/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/anaconda3/envs/tensorflow_1_14/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# Import necessary packages \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful references: \n",
    "# https://github.com/PacktPublishing/Hands-On-Natural-Language-Processing-with-Python/blob/master/Chapter08/02_example.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. PreProcessing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sepdata = pd.read_csv('wikihowSep.csv')[['headline','text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>\\nSell yourself first.</td>\n",
       "      <td>Before doing anything else, stop and sum up y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>\\nRead the classics before 1600.</td>\n",
       "      <td>Reading the classics is the very first thing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>\\nJoin online artist communities.</td>\n",
       "      <td>Depending on what scale you intend to sell yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>\\nMake yourself public.</td>\n",
       "      <td>Get yourself out there as best as you can by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>\\nBlog about your artwork.</td>\n",
       "      <td>Given the hundreds of free blogging websites,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            headline  \\\n",
       "0             \\nSell yourself first.   \n",
       "1   \\nRead the classics before 1600.   \n",
       "2  \\nJoin online artist communities.   \n",
       "3            \\nMake yourself public.   \n",
       "4         \\nBlog about your artwork.   \n",
       "\n",
       "                                                text  \n",
       "0   Before doing anything else, stop and sum up y...  \n",
       "1   Reading the classics is the very first thing ...  \n",
       "2   Depending on what scale you intend to sell yo...  \n",
       "3   Get yourself out there as best as you can by ...  \n",
       "4   Given the hundreds of free blogging websites,...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sepdata.head()\n",
    "# We only care about 'headline' and 'text' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sepdata_v1 = sepdata.dropna(axis=0).reset_index(drop=True) \n",
    "# Get rid of any NA rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1387290"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sepdata_v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re #for regex search purposes          \n",
    "from nltk.corpus import stopwords #stopwords that are provided to us via nltk \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of contractions that we will map to \n",
    "\n",
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "\n",
    "                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changes here: \n",
    "# Do not get rid of stopwords \n",
    "# Do not get rid of short words. \n",
    "\n",
    "def text_cleaner(text):\n",
    "    # Step 0: Convert to string in case a float or int is found.\n",
    "    newString = str(text)\n",
    "    # Step 1: Lower case the text \n",
    "    newString = newString.lower()\n",
    "    # Step 2: Get rid of commas\n",
    "    newString = re.sub(r'\\([^)]*\\)', \"\", newString)\n",
    "    # Step 3: Get rid of quotations \n",
    "    #newString = re.sub('\"',\"\", newString)\n",
    "    # Step 4: get rid of contractions with our contraction mapping \n",
    "    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])  \n",
    "    # Step 5: get rid of the \\n stuff \n",
    "    newString = re.sub(r\"'s\\n\",\"\",newString)\n",
    "    # Step 6: anything that is a number, get rid of it \n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n",
    "    tokens = newString.split()\n",
    "    remaining = []\n",
    "    for i in tokens: \n",
    "        if len(i)>=1: \n",
    "            remaining.append(i)\n",
    "    # Step 7: Tokenize everything first and keep the words that are not stop words \n",
    "    # Also keep only words that are greater than or equal to 3 characters long \n",
    "    #tokens = [w for w in newString.split() if not w in stop_words]\n",
    "    #long_words=[]\n",
    "    #for i in tokens:\n",
    "    #    if len(i)>=3: #removing short words\n",
    "    #        long_words.append(i)   \n",
    "    return (\" \".join(remaining)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'before doing anything else stop and sum up yourself as an artist now think about how to translate that to an online profile be it the few words twitter allows you or an entire page of indulgence that your own website would allow you bring out the most salient features of your creativity your experience your passion and your reasons for painting make it clear to readers why you are an artist who loves art produces high quality art and is a true champion of art if you are not great with words find a friend who can help you with this really important aspect of selling online the establishment of your credibility and reliability'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sepdata_v1['text'][0:5].apply(text_cleaner)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changes here: \n",
    "# keep all words (previously, I got rid of words that were only 1 character long) \n",
    "\n",
    "def headline_cleaner(text):\n",
    "    # Step 0: Convert to string in case a float or int is found.\n",
    "    newString = str(text)\n",
    "    # Step 1: remove quotations \n",
    "    newString = re.sub('\"','', newString)\n",
    "    # Step 2: look up contractions \n",
    "    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")]) \n",
    "    # Step 3: Get rid of the \\n stuff \n",
    "    newString = re.sub(r\"'s\\n\",\"\",newString)\n",
    "    # Step 4: Get rid of numbers or anything not in the alphabet\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString)\n",
    "    # Step 5: Lower case \n",
    "    newString = newString.lower()\n",
    "    tokens=newString.split()\n",
    "    # Step 6: keep words that are greater than 1 character long \n",
    "    remaining=[]\n",
    "    for i in tokens:\n",
    "        if len(i)>=1:                                 \n",
    "            remaining.append(i) \n",
    "    return (\" \".join(remaining)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sell yourself first'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sepdata_v1['headline'][0:5].apply(headline_cleaner)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = sepdata_v1['text'].apply(text_cleaner) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_y = sepdata_v1['headline'].apply(headline_cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = pd.concat([cleaned_data, cleaned_y], axis=1)\n",
    "clean_data.columns = ['text','headline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>before doing anything else stop and sum up you...</td>\n",
       "      <td>sell yourself first</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>reading the classics is the very first thing y...</td>\n",
       "      <td>read the classics before</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>depending on what scale you intend to sell you...</td>\n",
       "      <td>join online artist communities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>get yourself out there as best as you can by a...</td>\n",
       "      <td>make yourself public</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>given the hundreds of free blogging websites y...</td>\n",
       "      <td>blog about your artwork</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  before doing anything else stop and sum up you...   \n",
       "1  reading the classics is the very first thing y...   \n",
       "2  depending on what scale you intend to sell you...   \n",
       "3  get yourself out there as best as you can by a...   \n",
       "4  given the hundreds of free blogging websites y...   \n",
       "\n",
       "                         headline  \n",
       "0             sell yourself first  \n",
       "1        read the classics before  \n",
       "2  join online artist communities  \n",
       "3            make yourself public  \n",
       "4         blog about your artwork  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "# Functions to build encoder and decoder embeddings later \n",
    "##### \n",
    "\n",
    "# Building top vocab \n",
    "def count_words(words_dict, text):\n",
    "    for sentence in text:\n",
    "        for word in sentence.split():\n",
    "            if word not in words_dict:\n",
    "                words_dict[word] = 1\n",
    "            else:\n",
    "                words_dict[word] += 1\n",
    "\n",
    "def convert_text_to_ids(text, word2int_dict, eos=False): \n",
    "    output = []\n",
    "    for item in text:\n",
    "        item2int=[]\n",
    "        for word in item.split(): \n",
    "            if word in word2int_dict: \n",
    "                item2int.append(word2int_dict[word]) \n",
    "            else: \n",
    "                item2int.append(word2int_dict[TOKEN_UNK])\n",
    "        if eos: \n",
    "            item2int.append(word2int_dict[TOKEN_EOS])\n",
    "        output.append(item2int)\n",
    "    return output\n",
    "\n",
    "TOKEN_GO = '<GO>'\n",
    "TOKEN_EOS = '<EOS>'\n",
    "TOKEN_PAD = '<PAD>'\n",
    "TOKEN_UNK = '<UNK>'\n",
    "# These are special tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained embeddings and build word vector matrix \n",
    "\n",
    "# Vivian: originally using 300D but too many parameters...\n",
    "\n",
    "def build_word_vector_matrix(vector_file):\n",
    "    embedding_index = {}\n",
    "    f = open(vector_file)\n",
    "    for line in f: \n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype = 'float32') \n",
    "        embedding_index[word] = coefs\n",
    "    f.close() \n",
    "    return embedding_index\n",
    "\n",
    "# Replace the path here to point to the glove.6B.50d.txt vectors file on your system\n",
    "embeddings_index = build_word_vector_matrix('glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_seq(texts, MAX_NB_WORDS, EMBEDDING_DIM, isSummary): \n",
    "    # -- Build word count dictionary: \n",
    "    word_counts_dict = {}\n",
    "    count_words(word_counts_dict, texts) \n",
    "    print(\"Total words in Vocabulary:\", len(word_counts_dict))\n",
    "    \n",
    "    # -- sort and return top MAX_NB_WORDS \n",
    "    sorted_x = sorted(word_counts_dict.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    sorted_dict_test = collections.OrderedDict(sorted_x)\n",
    "    d = collections.Counter(sorted_dict_test)\n",
    "    word_dict = dict(d.most_common(VOCAB_SIZE))\n",
    "    \n",
    "    # -- Build word to int dictionary \n",
    "    word2int = {} \n",
    "    value = 0\n",
    "    for word, count in word_dict.items():\n",
    "        if word in embeddings_index:\n",
    "            word2int[word] = value\n",
    "            value += 1\n",
    "    special_codes = [TOKEN_UNK,TOKEN_PAD,TOKEN_EOS,TOKEN_GO]\n",
    "    for code in special_codes:\n",
    "        word2int[code] = len(word2int)\n",
    "        \n",
    "    # -- Build int to word dictionary \n",
    "    int2word = {}\n",
    "    for word, value in word2int.items():\n",
    "        int2word[value] = word\n",
    "    \n",
    "    # -- Build word_emb_matrix \n",
    "    word_emb_matrix = np.zeros((len(word2int), EMBEDDING_DIM), dtype=np.float32) \n",
    "    for word, i in word2int.items():\n",
    "        if word in embeddings_index:\n",
    "            # put in the embedding for the word \n",
    "            word_emb_matrix[i] = embeddings_index[word]\n",
    "        else:\n",
    "            # if word is not found, put in a random embedding \n",
    "            new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "            word_emb_matrix[i] = new_embedding\n",
    "    print(\"Length of word embeddings: \", len(word_emb_matrix))\n",
    "    \n",
    "    # -- Finally, convert doc to the entire sequence \n",
    "    if isSummary==True: \n",
    "        doc_as_ids = convert_text_to_ids(texts, word2int) #summaries do not get the EOS tag at the end \n",
    "    else: \n",
    "        doc_as_ids = convert_text_to_ids(texts, word2int, eos=True) \n",
    "    return (doc_as_ids, word_emb_matrix, word2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in Vocabulary: 68630\n",
      "Length of word embeddings:  39475\n",
      "Total words in Vocabulary: 156212\n",
      "Length of word embeddings:  39896\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 40000\n",
    "embedding_dim = 300 \n",
    "\n",
    "y_data, decoder_emb, y_word_index = text_to_seq(clean_data['headline'], VOCAB_SIZE, embedding_dim, isSummary = True)\n",
    "X_data, encoder_emb, x_word_index = text_to_seq(clean_data['text'], VOCAB_SIZE, embedding_dim, isSummary = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39896, 300)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39896"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39475, 300)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39475"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to deal with padding \n",
    "\n",
    "# Arbitrary lengths decided by exploratory work...can change this later as we see fit \n",
    "max_text_length = 100\n",
    "max_summary_length = 30\n",
    "\n",
    "def pad_sequences(textids_seq, summaryids_seq, x_word_index, y_word_index, max_text_length, max_summary_length): \n",
    "    padded_text = []\n",
    "    for item in textids_seq: \n",
    "        itemlen = len(item) \n",
    "        if itemlen<max_text_length: \n",
    "            # sequence is shorter than max_text_length \n",
    "            padded_text.append(item + [x_word_index[TOKEN_PAD]]*(max_text_length - itemlen))\n",
    "            #padded_text.append(np.array(item + [word2int[TOKEN_PAD]]*(max_text_length - itemlen)))\n",
    "        else: \n",
    "            # sequence is longer than max_text_length \n",
    "            #sublist = item[:(max_text_length-1)]\n",
    "            padded_text.append(item[:(max_text_length-1)] + [x_word_index[TOKEN_EOS]])\n",
    "            #padded_text.append(np.array(item[:(max_text_length-1)] + [word2int[TOKEN_EOS]]))\n",
    "    padded_summary = []\n",
    "    for item in summaryids_seq: \n",
    "        itemlen = len(item)\n",
    "        if itemlen<max_summary_length: \n",
    "            # sequence is shorter than max_summary_length \n",
    "            padded_summary.append(item + [y_word_index[TOKEN_PAD]]*(max_summary_length - itemlen))\n",
    "            #padded_summary.append(np.array(item + [word2int[TOKEN_PAD]]*(max_summary_length - itemlen)))\n",
    "        else: \n",
    "            # sequence is longer than max_summary_length \n",
    "            # also recall from previous: no EOS tag for summaries \n",
    "            padded_summary.append(item[:(max_summary_length)])\n",
    "            #padded_summary.append(np.array(item[:(max_summary_length)]))\n",
    "    return(np.array(padded_text), np.array(padded_summary))\n",
    "\n",
    "X_data_padded, y_data_padded = pad_sequences(X_data, y_data,x_word_index, y_word_index, max_text_length, max_summary_length)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   69,   208,   308,   394,   363,     4,  3760,    37,    86,\n",
       "          21,    30,  3013,   213,   153,    41,    65,     1,  6484,\n",
       "          12,     1,    30,   226,  1467,    15,    10,     0,   151,\n",
       "         425,  2265,   697,     2,     9,    30,   613,   336,     6,\n",
       "       19728,    12,     5,   155,   415,   127,   197,     2,   292,\n",
       "          39,     0,    79, 39892,  1486,     6,     5,  4593,     5,\n",
       "         318,     5,  3911,     4,     5,  1195,    11,  1847,    26,\n",
       "          10,   408,     1,  2596,   405,     2,    14,    30,  3013,\n",
       "          97,  3635,  1287,  4137,   203,   641,  1287,     4,     8,\n",
       "           3,   996, 11312,     6,  1287,    13,     2,    14,    20,\n",
       "         206,    18,   425,    73,     3,   241,    97,    16,    52,\n",
       "       39894])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_padded[0]\n",
    "#print(len(X_data_padded[0])) #!00 long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39894"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_word_index[TOKEN_EOS] #note the EOS tag at the end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  976,    60,    93, 39472, 39472, 39472, 39472, 39472, 39472,\n",
       "       39472, 39472, 39472, 39472, 39472, 39472, 39472, 39472, 39472,\n",
       "       39472, 39472, 39472, 39472, 39472, 39472, 39472, 39472, 39472,\n",
       "       39472, 39472, 39472])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data_padded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  150,     0,  9609,    79, 39472, 39472, 39472, 39472, 39472,\n",
       "       39472, 39472, 39472, 39472, 39472, 39472, 39472, 39472, 39472,\n",
       "       39472, 39472, 39472, 39472, 39472, 39472, 39472, 39472, 39472,\n",
       "       39472, 39472, 39472])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data_padded[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39472"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_word_index[TOKEN_PAD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1387290, 30)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  976,    60,    93, ..., 39472, 39472, 39472],\n",
       "       [  150,     0,  9609, ..., 39472, 39472, 39472],\n",
       "       [  461,   217,  3287, ..., 39472, 39472, 39472],\n",
       "       ...,\n",
       "       [   45,     8,     0, ..., 39472, 39472, 39472],\n",
       "       [   45,     8,     0, ..., 39472, 39472, 39472],\n",
       "       [   45,     8, 39471, ..., 39472, 39472, 39472]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Build Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note to self: Most of these, trying to stay as close to Vivek's model naming so we can compare "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Dense, Concatenate, TimeDistributed, Bidirectional, Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from attention_keras.layers.attention import AttentionLayer\n",
    "\n",
    "from tensorflow.keras import backend as K \n",
    "K.clear_session() \n",
    "hidden_units = 200 #Paper mentions 600 hidden units, but we can change this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39896"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_word_index) # given that I have the tokens in all of my word indexes, i think I can just take len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_embedding_layer = Embedding(len(x_word_index),\n",
    "                            embedding_dim,\n",
    "                            weights=[encoder_emb],\n",
    "                            input_length=max_text_length,\n",
    "                            trainable=False,\n",
    "                            name='EncoderEmbeddingLayer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_embedding_layer = Embedding(len(y_word_index),\n",
    "                            embedding_dim,\n",
    "                            weights=[decoder_emb],\n",
    "                            input_length=max_summary_length,\n",
    "                            trainable=False,\n",
    "                            name='DecoderEmbeddingLayer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/tensorflow_1_14/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# Encoder \n",
    "\n",
    "# Encoder input \n",
    "# 2D (sequence_length, None), where sequence length is the MAX_LEN unified by padding in preprocessing\n",
    "encoder_inputs = Input(shape=(max_text_length,), name=\"EncoderInput\") \n",
    "enc_emb = enc_embedding_layer(encoder_inputs) \n",
    "\n",
    "\n",
    "#LSTM 1 \n",
    "encoder_lstm1 = LSTM(hidden_units,return_sequences=True,return_state=True, name='EncLSTM1') \n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb) \n",
    "\n",
    "#LSTM 2 \n",
    "encoder_lstm2 = LSTM(hidden_units,return_sequences=True,return_state=True, name='EncLSTM2') \n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1) \n",
    "\n",
    "#LSTM 3 \n",
    "encoder_lstm3=LSTM(hidden_units, return_state=True, return_sequences=True, name='EncLSTM3') \n",
    "encoder_output3, state_h3, state_c3= encoder_lstm3(encoder_output2) \n",
    "\n",
    "#LSTM 4 \n",
    "encoder_lstm4=LSTM(hidden_units, return_state=True, return_sequences=True, name='EncLSTM4') \n",
    "encoder_outputs, state_h, state_c= encoder_lstm4(encoder_output3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder \n",
    "\n",
    "decoder_inputs = Input(shape=(None,), name = 'DecoderInput') \n",
    "#dec_emb_layer = Embedding(y_voc_size, latent_dim,trainable=True) \n",
    "dec_emb = dec_embedding_layer(decoder_inputs) \n",
    "\n",
    "#LSTM using encoder_states as initial state\n",
    "decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True, name='DecLSTM1') \n",
    "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c]) \n",
    "\n",
    "#Attention Layer\n",
    "attn_layer = AttentionLayer(name='attention_layer') \n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs]) \n",
    "\n",
    "# Concat attention output and decoder LSTM output \n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "#Dense layer\n",
    "decoder_dense = TimeDistributed(Dense(len(y_word_index)+1, activation='softmax')) \n",
    "decoder_outputs = decoder_dense(decoder_concat_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "EncoderInput (InputLayer)       [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "EncoderEmbeddingLayer (Embeddin (None, 100, 300)     11968800    EncoderInput[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "EncLSTM1 (LSTM)                 [(None, 100, 200), ( 400800      EncoderEmbeddingLayer[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "EncLSTM2 (LSTM)                 [(None, 100, 200), ( 320800      EncLSTM1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "DecoderInput (InputLayer)       [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "EncLSTM3 (LSTM)                 [(None, 100, 200), ( 320800      EncLSTM2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "DecoderEmbeddingLayer (Embeddin (None, None, 300)    11842500    DecoderInput[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "EncLSTM4 (LSTM)                 [(None, 100, 200), ( 320800      EncLSTM3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "DecLSTM1 (LSTM)                 [(None, None, 200),  400800      DecoderEmbeddingLayer[0][0]      \n",
      "                                                                 EncLSTM4[0][1]                   \n",
      "                                                                 EncLSTM4[0][2]                   \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer ((None, None, 200),  80200       EncLSTM4[0][0]                   \n",
      "                                                                 DecLSTM1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, None, 400)    0           DecLSTM1[0][0]                   \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, None, 39476)  15829876    concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 41,485,376\n",
      "Trainable params: 17,674,076\n",
      "Non-trainable params: 23,811,300\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/tensorflow_1_14/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "2000/2000 [==============================] - 178s 89ms/sample - loss: 3.9606\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x16af13048>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([X_data_padded[0:2000],y_data_padded[0:2000]], \n",
    "                  y_data_padded[0:2000],\n",
    "                  epochs=1,\n",
    "                  batch_size=50)\n",
    "\n",
    "\n",
    "# history=model.fit([x_tr,y_tr[:,:-1]], y_tr.reshape(y_tr.shape[0],y_tr.shape[1], 1)[:,1:] ,epochs=50,callbacks=[es],batch_size=512, validation_data=([x_val,y_val[:,:-1]], y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow1_14",
   "language": "python",
   "name": "tensorflow_1_14"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
