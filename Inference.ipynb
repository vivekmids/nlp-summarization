{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from matplotlib import pyplot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = pickle.load(open( 'history.pkl' , \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "pyplot.plot(hist['loss'], label='train') \n",
    "pyplot.plot(hist['val_loss'], label='test') \n",
    "pyplot.legend() \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data.\n",
      "Training data loaded. Shape is (1387290, 2)\n",
      "Starting to build the embedding index.\n",
      "Built embeddings index. Found 400000 word vectors.\n",
      "Building text\n",
      "Found 212813 unique tokens.\n",
      "Text built. Lengths of x_data, encoder_emb, x_word_index, x_index_word are [1387290, 212814, 212813, 212813]\n",
      "Building headlines \n",
      "Found 78682 unique tokens.\n",
      "Headlines built. Lengths of y_data, decoder_emb, y_word_index, y_index_word are [1387290, 78683, 78682, 78682]\n",
      "Test Train Dev split done. Length of x_train, y_train,x_dev, y_dev, x_test, y_test are [971103, 971103, 278845, 278845, 137342, 137342]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import pickle\n",
    "from time import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "#cleanfilename = 'cleandatamini.pkl'\n",
    "cleanfilename = 'cleandata.pkl'\n",
    "modelfilename = 'model.json'\n",
    "weightsfilename = 'model.h5'\n",
    "historyfilename = 'history.pkl'\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "print('Loading training data.')\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "clean_data = pickle.load( open( cleanfilename , \"rb\" ) )\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "print('Training data loaded. Shape is', clean_data.shape)\n",
    "\n",
    "\n",
    "# In[27]:\n",
    "\n",
    "\n",
    "print('Starting to build the embedding index.')\n",
    "\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('data/glove/glove.6B/glove.6B.300d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "\n",
    "print('Built embeddings index. Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "\n",
    "# In[30]:\n",
    "\n",
    "\n",
    "def doc2seq(texts, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM):\n",
    "    tokenizer = Tokenizer(filters='\"#$%&()*+-/<=>@[\\\\]^_`{|}~\\t\\n')\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    index_word = tokenizer.index_word\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding = 'post')\n",
    "    \n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return data, embedding_matrix, word_index, index_word\n",
    "\n",
    "\n",
    "# In[31]:\n",
    "\n",
    "\n",
    "print('Building text')\n",
    "\n",
    "\n",
    "# In[32]:\n",
    "\n",
    "\n",
    "MAX_TEXT_LENGTH = 100\n",
    "EMBEDDING_DIM = 300\n",
    "data = clean_data.text\n",
    "\n",
    "x_data, encoder_emb, x_word_index, x_index_word = doc2seq(data, MAX_TEXT_LENGTH, EMBEDDING_DIM)\n",
    "\n",
    "\n",
    "# In[33]:\n",
    "\n",
    "\n",
    "print('Text built. Lengths of x_data, encoder_emb, x_word_index, x_index_word are', \n",
    "      list(map(lambda a:len(a), [x_data, encoder_emb, x_word_index, x_index_word ])))\n",
    "\n",
    "\n",
    "# In[34]:\n",
    "\n",
    "\n",
    "print('Building headlines ')\n",
    "\n",
    "\n",
    "# In[35]:\n",
    "\n",
    "\n",
    "MAX_HEADLINE_LENGTH = 30\n",
    "EMBEDDING_DIM = 300\n",
    "data = clean_data.headline\n",
    "\n",
    "y_data, decoder_emb, y_word_index, y_index_word = doc2seq(data, MAX_HEADLINE_LENGTH, EMBEDDING_DIM)\n",
    "\n",
    "\n",
    "# In[36]:\n",
    "\n",
    "\n",
    "print('Headlines built. Lengths of y_data, decoder_emb, y_word_index, y_index_word are', \n",
    "      list(map(lambda a:len(a), [y_data, decoder_emb, y_word_index, y_index_word])))\n",
    "\n",
    "\n",
    "# In[37]:\n",
    "\n",
    "\n",
    "x_train, x_test_temp, y_train, y_test_temp = train_test_split(x_data, y_data, \n",
    "                                                            test_size=0.3, random_state=0) \n",
    "\n",
    "\n",
    "# In[38]:\n",
    "\n",
    "\n",
    "x_dev, x_test, y_dev, y_test = train_test_split(x_test_temp, y_test_temp, \n",
    "                                                            test_size=0.33, random_state=0) \n",
    "\n",
    "\n",
    "# In[39]:\n",
    "\n",
    "\n",
    "print('Test Train Dev split done. Length of x_train, y_train,x_dev, y_dev, x_test, y_test are', \n",
    "     list(map(len, [x_train, y_train,x_dev, y_dev, x_test, y_test])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Dense, Concatenate, TimeDistributed, Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "from attention_keras.layers.attention import AttentionLayer\n",
    "from tensorflow.keras.utils import multi_gpu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TEXT_LENGTH = 100\n",
    "MAX_HEADLINE_LENGTH = 30\n",
    "EMBEDDING_DIM = 300\n",
    "hidden_units = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_embedding_layer = Embedding(len(x_word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[encoder_emb],\n",
    "                            input_length=MAX_TEXT_LENGTH,\n",
    "                            trainable=False,\n",
    "                            name='EncoderEmbeddingLayer')\n",
    "\n",
    "dec_embedding_layer = Embedding(len(y_word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[decoder_emb],\n",
    "                            input_length=MAX_HEADLINE_LENGTH,\n",
    "                            trainable=False,\n",
    "                            name='DecoderEmbeddingLayer')\n",
    "\n",
    "\n",
    "# Encoder \n",
    "\n",
    "# Encoder input \n",
    "# 2D (sequence_length, None), where sequence length is the MAX_LEN unified by padding in preprocessing\n",
    "encoder_inputs = Input(shape=(MAX_TEXT_LENGTH,), name=\"EncoderInput\") \n",
    "enc_emb = enc_embedding_layer(encoder_inputs) \n",
    "\n",
    "\n",
    "#LSTM 1 \n",
    "encoder_lstm1 = LSTM(hidden_units,return_sequences=True,return_state=True, name='EncLSTM1') \n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb) \n",
    "\n",
    "#LSTM 2 \n",
    "encoder_lstm2 = LSTM(hidden_units,return_sequences=True,return_state=True, name='EncLSTM2') \n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1) \n",
    "\n",
    "#LSTM 3 \n",
    "encoder_lstm3=LSTM(hidden_units, return_state=True, return_sequences=True, name='EncLSTM3') \n",
    "encoder_output3, state_h3, state_c3= encoder_lstm3(encoder_output2) \n",
    "\n",
    "#LSTM 4 \n",
    "encoder_lstm4=LSTM(hidden_units, return_state=True, return_sequences=True, name='EncLSTM4') \n",
    "encoder_outputs, state_h, state_c= encoder_lstm4(encoder_output3) \n",
    "\n",
    "\n",
    "# Decoder \n",
    "\n",
    "decoder_inputs = Input(shape=(None,), name = 'DecoderInput') \n",
    "#dec_emb_layer = Embedding(y_voc_size, latent_dim,trainable=True) \n",
    "dec_emb = dec_embedding_layer(decoder_inputs) \n",
    "\n",
    "#LSTM using encoder_states as initial state\n",
    "decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True, name='DecLSTM1') \n",
    "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c]) \n",
    "\n",
    "#Attention Layer\n",
    "attn_layer = AttentionLayer(name='attention_layer') \n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs]) \n",
    "\n",
    "# Concat attention output and decoder LSTM output \n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "#Dense layer\n",
    "decoder_dense = TimeDistributed(Dense(len(y_word_index)+1, activation='softmax')) \n",
    "decoder_outputs = decoder_dense(decoder_concat_input) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "EncoderInput (InputLayer)       [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "EncoderEmbeddingLayer (Embeddin (None, 100, 300)     63844200    EncoderInput[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "EncLSTM1 (LSTM)                 [(None, 100, 300), ( 721200      EncoderEmbeddingLayer[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "EncLSTM2 (LSTM)                 [(None, 100, 300), ( 721200      EncLSTM1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "DecoderInput (InputLayer)       [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "EncLSTM3 (LSTM)                 [(None, 100, 300), ( 721200      EncLSTM2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "DecoderEmbeddingLayer (Embeddin (None, None, 300)    23604900    DecoderInput[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "EncLSTM4 (LSTM)                 [(None, 100, 300), ( 721200      EncLSTM3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "DecLSTM1 (LSTM)                 [(None, None, 300),  721200      DecoderEmbeddingLayer[0][0]      \n",
      "                                                                 EncLSTM4[0][1]                   \n",
      "                                                                 EncLSTM4[0][2]                   \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer ((None, None, 300),  180300      EncLSTM4[0][0]                   \n",
      "                                                                 DecLSTM1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, None, 600)    0           DecLSTM1[0][0]                   \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, None, 78683)  47288483    concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 138,523,883\n",
      "Trainable params: 51,074,783\n",
      "Non-trainable params: 87,449,100\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model inputs must come from `tf.keras.Input` (thus holding past layer metadata), they cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_3\" was not an Input tensor, it was generated by layer DecoderInput.\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: DecoderInput_8:0\n"
     ]
    }
   ],
   "source": [
    "parallel_model = multi_gpu_model(model, gpus = 2)\n",
    "parallel_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "parallel_model.load_weights(\"model_fd_111319_250units_punct.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model inputs must come from `tf.keras.Input` (thus holding past layer metadata), they cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_5\" was not an Input tensor, it was generated by layer DecoderInput.\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: DecoderInput_8:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model inputs must come from `tf.keras.Input` (thus holding past layer metadata), they cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_5\" was not an Input tensor, it was generated by layer DecoderInput.\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: DecoderInput_8:0\n"
     ]
    }
   ],
   "source": [
    "# encoder inference\n",
    "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# decoder inference\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(hidden_units,))\n",
    "decoder_state_input_c = Input(shape=(hidden_units,))\n",
    "decoder_hidden_state_input = Input(shape=(MAX_TEXT_LENGTH,hidden_units))\n",
    "\n",
    "# Get the embeddings of the decoder sequence\n",
    "dec_emb2= dec_embedding_layer(decoder_inputs)\n",
    "\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "#attention inference\n",
    "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "decoder_outputs2 = decoder_dense(decoder_inf_concat)\n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "[decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "[decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2summary(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0 ):\n",
    "            newString=newString+y_index_word[i]+' '\n",
    "    return newString\n",
    "\n",
    "def seq2text(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            newString=newString+x_index_word[i]+' '\n",
    "    return newString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "\n",
    "    # Chose the 'start' word as the first word of the target sequence\n",
    "    target_seq[0, 0] = y_word_index.get('START', 0)\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "        #print(output_tokens)\n",
    "        #print(h)\n",
    "        #print(c)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        #print(sampled_token_index)\n",
    "        sampled_token = y_index_word.get(sampled_token_index, '.')\n",
    "\n",
    "        if(sampled_token!='END'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "            # Exit condition: either hit max length or find stop word.\n",
    "            if (sampled_token == 'END' or len(decoded_sentence.split()) >= (MAX_HEADLINE_LENGTH-1)):\n",
    "                stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update internal states\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence_beam(input_seq, beam=3):\n",
    "    # Encode the input as state vectors.\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "\n",
    "    top_sentences = {}\n",
    "    \n",
    "    def top_tokens(last_token, out, h, c):\n",
    "        output_tokens, h_new, c_new = decoder_model.predict([[last_token]] + [out, h, c])\n",
    "        top_token_indexes = np.argsort(output_tokens[0, -1, :])[-beam:]\n",
    "        top_probabilities = output_tokens[0,-1, top_token_indexes]\n",
    "        return top_token_indexes, top_probabilities, h_new, c_new\n",
    "        \n",
    "    #first set of tokens when feeding encoder states and 0 as the first token to the decoder.\n",
    "    first_tokens, first_probabilities, h, c = top_tokens(0, e_out, e_h, e_c)\n",
    "    for first_token, first_probability in zip(first_tokens, first_probabilities):\n",
    "        #initialize top sentences, their corresponding probabilities and states\n",
    "        top_sentences[y_index_word.get(first_token, '')] = (first_probability, e_h, e_c)\n",
    "    \n",
    "    \n",
    "    #loop to iterate over next tokens\n",
    "    len = 1\n",
    "    while len < MAX_HEADLINE_LENGTH:\n",
    "        candidate_sentences = {}\n",
    "        for sentence, (probability, h, c) in top_sentences.items():\n",
    "            last_word = sentence.split()[-1] #pick the last word in the sentence as next word\n",
    "            if(last_word != '.'):\n",
    "                token = y_word_index.get(last_word, 0) \n",
    "                next_tokens, next_probabilities, h_next, c_next = top_tokens(token, e_out, h, c)\n",
    "                for next_token, next_probability in zip(next_tokens, next_probabilities):\n",
    "                    new_sentence = sentence.strip() + ' ' + y_index_word.get(next_token, '')\n",
    "                    candidate_sentences[new_sentence.strip()] = (probability * next_probability, h_next, c_next)\n",
    "            else:\n",
    "                candidate_sentences[sentence] = (probability, h, c)\n",
    "\n",
    "        #print('Candidate sentences')\n",
    "        #print(candidate_sentences.keys())\n",
    "        \n",
    "        #remove low probability candidates\n",
    "        low_probability_candidates = sorted(candidate_sentences, key=lambda k: candidate_sentences.get(k)[0])[:-beam]\n",
    "        for low_probability_candidate in low_probability_candidates:\n",
    "            candidate_sentences.pop(low_probability_candidate)\n",
    "        \n",
    "        #Now all candidates left have highest probabilities.\n",
    "        top_sentences = candidate_sentences\n",
    "        len = len + 1\n",
    "        #print('Sentences at the bottom of the loop')\n",
    "        #print(top_sentences.keys())\n",
    "        \n",
    "\n",
    "    return top_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rinse'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'rinse'.split()[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "##### helpful functions to help with formatting when printing: \n",
    "############\n",
    "def printmd(string):\n",
    "    display(Markdown(string)) #just a pretty print \n",
    "\n",
    "############\n",
    "##### function for BLEU \n",
    "#############\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "# reference: https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\n",
    "\n",
    "def calc_indiv_BLEU(id_text, text_df, headline_df): \n",
    "    # This function will take the following as inputs: \n",
    "    # id_text: the index you are interested in \n",
    "    # gen_text_df: the sequences that hold the full text \n",
    "    # headline_df: the sequences that hold the headline text \n",
    "    \n",
    "    # -- Step 1: generate the decoded sequence from a given sample of text\n",
    "    gen_output = decode_sequence(text_df[id_text].reshape(1,-1))\n",
    "    split_output = gen_output.split(\" \")\n",
    "    candidate = [item for item in split_output if (item!=\".\" and item!=\"\")] #get rid of empty spaces and periods \n",
    "    # -- Step 2: generate the true headline summary from our labelled headline text\n",
    "    gen_ref = seq2summary(headline_df[id_text])\n",
    "    split_ref = gen_ref.split(\" \")\n",
    "    #get rid of empty spaces and periods (there shouldn't be any as we already cleaned the headline, but just in case)\n",
    "    reference = [item for item in split_ref if (item!=\".\" and item!=\"\")] \n",
    "    # -- Step 3: calculate BLEU \n",
    "    score = sentence_bleu(gen_ref, gen_output, weights=(1, 0, 0, 0))\n",
    "    # we can alternate weights for cumulative scores afterwards \n",
    "    # For now, BLEU is based on unigram counts \n",
    "    return(score)\n",
    "\n",
    "###############\n",
    "##### function for rouge \n",
    "############### \n",
    "# PULL METRICS.PY FILE!! \n",
    "from metrics import rouge_n_sentence_level\n",
    "# metrics.py is taken from https://github.com/neural-dialogue-metrics/rouge\n",
    "\n",
    "# Other useful links to keep in mind: \n",
    "# https://stackoverflow.com/questions/38045290/text-summarization-evaluation-bleu-vs-rouge\n",
    "\n",
    "def calc_indiv_rouge(id_text, text_df, headline_df, rouge_n): \n",
    "    # This function will take the following as inputs: \n",
    "    # id_text: the index you are interested in \n",
    "    # gen_text_df: the sequences that hold the full text \n",
    "    # headline_df: the sequences that hold the headline text \n",
    "    \n",
    "    # -- Step 1: generate the decoded sequence from a given sample of text\n",
    "    gen_output = decode_sequence(text_df[id_text].reshape(1,-1))\n",
    "    split_output = gen_output.split(\" \")\n",
    "    candidate = [item for item in split_output if (item!=\".\" and item!=\"\")] #get rid of empty spaces and periods \n",
    "    # -- Step 2: generate the true headline summary from our labelled headline text\n",
    "    gen_ref = seq2summary(headline_df[id_text])\n",
    "    split_ref = gen_ref.split(\" \")\n",
    "    #get rid of empty spaces and periods (there shouldn't be any as we already cleaned the headline, but just in case)\n",
    "    reference = [item for item in split_ref if (item!=\".\" and item!=\"\")] \n",
    "    # -- Step 3: calculate rouge\n",
    "    recall, precision, rouge = rouge_n_sentence_level(candidate, reference, rouge_n)\n",
    "    # rouge is actually an f-score of the recall and precision \n",
    "    return(recall, precision, rouge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "# Evaluation print-out example \n",
    "############# \n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Generated summary:** attach the vent hose to the vent . . . . . . . . . . . . . . . . . . . . . ."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Original summary:**make sure you have a compatible exhaust vent . "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Text:**the vent on your dryer needs to match up with the one in the wall in your laundry room . most exhaust vents are inches . cm in diameter . if the vent in your dryer does not match the one in your wall , you should be able to purchase a vent adapter or transition pipe at a hardware or home supply store . "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**BLEU score(Unigram):** 0.14285714285714285"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**ROUGE-**1**-Recall:** 0.125"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**ROUGE-**1**-Precision:** 0.14285714285714285"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**ROUGE-**1**-Fscore:** 0.13333333333333333"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Generated summary:** swing your arms . . . . . . . . . . . . . . . . . . . . . . . . . ."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Original summary:**turn your back knee towards the ball as your front heel touches down . "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Text:**this initiates a strong , powerful swing . as you start your swing your front foot must be planted firmly on the ground or you will lose balance and power . "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**BLEU score(Unigram):** 0.14705882352941177"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**ROUGE-**1**-Recall:** 0.07692307692307693"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**ROUGE-**1**-Precision:** 0.3333333333333333"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**ROUGE-**1**-Fscore:** 0.125"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Generated summary:** rinse the area with water . . . . . . . . . . . . . . . . . . . . . . . ."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Original summary:**spray with a garden hose . "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Text:**use a garden hose or bucket of water to rinse off the cleaner . make sure you’ve rinsed it off thoroughly . a chalky film will develop on the awning if the cleaner hasn’t been washed off . allow it to air dry . do not pressure wash an awning . pressure washing can cause damage . "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**BLEU score(Unigram):** 0.14864864864864866"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**ROUGE-**1**-Recall:** 0.2"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**ROUGE-**1**-Precision:** 0.2"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**ROUGE-**1**-Fscore:** 0.20000000000000004"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Generated summary:** be patient . . . . . . . . . . . . . . . . . . . . . . . . . . ."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Original summary:**thank the city council when done making your comments . "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Text:**conclude when your time is up , whether you have completed your thoughts or not . it is generally all right to finish your sentence or say thank you after the timer goes off , but do not keep going any longer than that . "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**BLEU score(Unigram):** 0.1076923076923077"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**ROUGE-**1**-Recall:** 0.0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**ROUGE-**1**-Precision:** 0.0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**ROUGE-**1**-Fscore:** 0.0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Generated summary:** be positive . . . . . . . . . . . . . . . . . . . . . . . . . . ."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Original summary:**try a friends audit . "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Text:**when was the last time you did a valuable friends audit what do you mean i hear you cry . i mean have you ever taken the time to re evaluate your friendships are there people around you that are constantly negative , pour cold water over your positive ideas , are miserable and fickle are these the kind of people that can help you to grow in confidence challenge be honest about the value of the friends you have and make some bold decisions . reason to remove yourself from negative influences and environments . "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**BLEU score(Unigram):** 0.0909090909090909"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**ROUGE-**1**-Recall:** 0.0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**ROUGE-**1**-Precision:** 0.0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**ROUGE-**1**-Fscore:** 0.0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(10,15):\n",
    "    printmd(\"**Generated summary:**\"+decode_sequence(x_train[i].reshape(1,-1)))\n",
    "    printmd(\"**Original summary:**\"+seq2summary(y_train[i]))\n",
    "    printmd(\"**Text:**\"+seq2text(x_train[i]))\n",
    "    printmd(\"**BLEU score(Unigram):** \"+str(calc_indiv_BLEU(i, x_train, y_train)))\n",
    "    rouge_n = 1 #this can be edited pending how we decide to evaluate ROUGE \n",
    "    printmd(\"**ROUGE-**\"+str(rouge_n)+\"**-Recall:** \"+str(calc_indiv_rouge(i,x_train,y_train,rouge_n)[0]))\n",
    "    printmd(\"**ROUGE-**\"+str(rouge_n)+\"**-Precision:** \"+str(calc_indiv_rouge(i,x_train,y_train,rouge_n)[1]))\n",
    "    printmd(\"**ROUGE-**\"+str(rouge_n)+\"**-Fscore:** \"+str(calc_indiv_rouge(i,x_train,y_train,rouge_n)[2]))\n",
    "    print('_________________________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_total= 0\n",
    "for i in range(len(x_train[:10])):\n",
    "    bleu_total += calc_indiv_BLEU(i, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLEU = bleu_total/len(x_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1393015078330715\n"
     ]
    }
   ],
   "source": [
    "print(BLEU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_recall_total, rouge_precision_total, rouge_f_total= 0,0,0\n",
    "for i in range(len(x_test[:20])):\n",
    "    rouge_recall, rouge_precision, rouge_f = calc_indiv_rouge(i, x_test, y_test,1)\n",
    "    rouge_recall_total += rouge_recall\n",
    "    rouge_precision_total += rouge_precision\n",
    "    rouge_f_total += rouge_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17671987734487735"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_recall_total/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23347222222222222"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_precision_total/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18876909038673745"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_f_total/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = decode_sequence_beam(x_train[20].reshape(1,-1), 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [(sen, prob) for sen, (prob, _, _) in res.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('serve distribute copies .', 0.0013948419),\n",
       " ('serve serve your spouse .', 0.000712558),\n",
       " ('serve complete the filing fee .', 0.00036260893),\n",
       " ('serve file a copy of the form .', 0.0001097252),\n",
       " ('serve file a copy of the complaint .', 6.931366e-05),\n",
       " ('serve complete a copy of the form .', 6.382113e-05),\n",
       " ('serve file a copy of the application .', 5.524569e-05),\n",
       " ('serve file a copy of your complaint .', 5.51969e-05),\n",
       " ('serve file a copy of your copy .', 5.3209587e-05),\n",
       " ('serve serve the other forms of service .', 5.154646e-05),\n",
       " ('serve serve notice on the other side .', 4.7479094e-05),\n",
       " ('serve file a copy of the copy .', 4.422074e-05),\n",
       " ('serve file a copy of the paperwork .', 4.162589e-05),\n",
       " ('serve file a copy of the forms .', 4.0115472e-05),\n",
       " ('serve file a copy of the other forms .', 2.025318e-05),\n",
       " ('serve serve a copy of the other forms .', 1.9995088e-05),\n",
       " ('serve file a copy of the proof of service .', 1.4671594e-05),\n",
       " ('serve serve a copy of the other forms of service .', 4.3389346e-06),\n",
       " ('serve serve a copy of the copy on the other side .', 3.0356684e-06),\n",
       " ('serve file a copy of the other forms of service .', 2.6802363e-06),\n",
       " ('serve file a copy of the copy on the server .', 1.844169e-06),\n",
       " ('serve serve a copy of the copy on the other parent .', 1.1647253e-06),\n",
       " ('serve attach a copy of the copy to the other forms .', 9.2890014e-07),\n",
       " ('serve serve a copy of the copy on the other side of the file .',\n",
       "  3.7431352e-08),\n",
       " ('serve serve a copy of the copy on the other side of the other parent .',\n",
       "  2.3665619e-08)]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(l, key = lambda x:-x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' rinse the area with water . . . . . . . . . . . . . . . . . . . . . . . .'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_sequence(x_train[12].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'turn your back knee towards the ball as your front heel touches down . '"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2summary(y_train[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this initiates a strong , powerful swing . as you start your swing your front foot must be planted firmly on the ground or you will lose balance and power . '"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2text(x_train[11])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
