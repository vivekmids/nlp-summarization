{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from matplotlib import pyplot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = pickle.load(open( 'history.pkl' , \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "pyplot.plot(hist['loss'], label='train') \n",
    "pyplot.plot(hist['val_loss'], label='test') \n",
    "pyplot.legend() \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data.\n",
      "Training data loaded. Shape is (1387290, 2)\n",
      "Starting to build the embedding index.\n",
      "Built embeddings index. Found 400000 word vectors.\n",
      "Building text\n",
      "Found 212813 unique tokens.\n",
      "Text built. Lengths of x_data, encoder_emb, x_word_index, x_index_word are [1387290, 212814, 212813, 212813]\n",
      "Building headlines \n",
      "Found 78682 unique tokens.\n",
      "Headlines built. Lengths of y_data, decoder_emb, y_word_index, y_index_word are [1387290, 78683, 78682, 78682]\n",
      "Test Train Dev split done. Length of x_train, y_train,x_dev, y_dev, x_test, y_test are [971103, 971103, 278845, 278845, 137342, 137342]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import pickle\n",
    "from time import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "#cleanfilename = 'cleandatamini.pkl'\n",
    "cleanfilename = 'cleandata.pkl'\n",
    "modelfilename = 'model.json'\n",
    "weightsfilename = 'model.h5'\n",
    "historyfilename = 'history.pkl'\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "print('Loading training data.')\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "clean_data = pickle.load( open( cleanfilename , \"rb\" ) )\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "print('Training data loaded. Shape is', clean_data.shape)\n",
    "\n",
    "\n",
    "# In[27]:\n",
    "\n",
    "\n",
    "print('Starting to build the embedding index.')\n",
    "\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('data/glove/glove.6B/glove.6B.300d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "\n",
    "print('Built embeddings index. Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "\n",
    "# In[30]:\n",
    "\n",
    "\n",
    "def doc2seq(texts, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM):\n",
    "    tokenizer = Tokenizer(filters='\"#$%&()*+-/<=>@[\\\\]^_`{|}~\\t\\n')\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    index_word = tokenizer.index_word\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding = 'post')\n",
    "    \n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return data, embedding_matrix, word_index, index_word\n",
    "\n",
    "\n",
    "# In[31]:\n",
    "\n",
    "\n",
    "print('Building text')\n",
    "\n",
    "\n",
    "# In[32]:\n",
    "\n",
    "\n",
    "MAX_TEXT_LENGTH = 100\n",
    "EMBEDDING_DIM = 300\n",
    "data = clean_data.text\n",
    "\n",
    "x_data, encoder_emb, x_word_index, x_index_word = doc2seq(data, MAX_TEXT_LENGTH, EMBEDDING_DIM)\n",
    "\n",
    "\n",
    "# In[33]:\n",
    "\n",
    "\n",
    "print('Text built. Lengths of x_data, encoder_emb, x_word_index, x_index_word are', \n",
    "      list(map(lambda a:len(a), [x_data, encoder_emb, x_word_index, x_index_word ])))\n",
    "\n",
    "\n",
    "# In[34]:\n",
    "\n",
    "\n",
    "print('Building headlines ')\n",
    "\n",
    "\n",
    "# In[35]:\n",
    "\n",
    "\n",
    "MAX_HEADLINE_LENGTH = 30\n",
    "EMBEDDING_DIM = 300\n",
    "data = clean_data.headline\n",
    "\n",
    "y_data, decoder_emb, y_word_index, y_index_word = doc2seq(data, MAX_HEADLINE_LENGTH, EMBEDDING_DIM)\n",
    "\n",
    "\n",
    "# In[36]:\n",
    "\n",
    "\n",
    "print('Headlines built. Lengths of y_data, decoder_emb, y_word_index, y_index_word are', \n",
    "      list(map(lambda a:len(a), [y_data, decoder_emb, y_word_index, y_index_word])))\n",
    "\n",
    "\n",
    "# In[37]:\n",
    "\n",
    "\n",
    "x_train, x_test_temp, y_train, y_test_temp = train_test_split(x_data, y_data, \n",
    "                                                            test_size=0.3, random_state=0) \n",
    "\n",
    "\n",
    "# In[38]:\n",
    "\n",
    "\n",
    "x_dev, x_test, y_dev, y_test = train_test_split(x_test_temp, y_test_temp, \n",
    "                                                            test_size=0.33, random_state=0) \n",
    "\n",
    "\n",
    "# In[39]:\n",
    "\n",
    "\n",
    "print('Test Train Dev split done. Length of x_train, y_train,x_dev, y_dev, x_test, y_test are', \n",
    "     list(map(len, [x_train, y_train,x_dev, y_dev, x_test, y_test])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Dense, Concatenate, TimeDistributed, Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "from attention_keras.layers.attention import AttentionLayer\n",
    "from tensorflow.keras.utils import multi_gpu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TEXT_LENGTH = 100\n",
    "MAX_HEADLINE_LENGTH = 30\n",
    "EMBEDDING_DIM = 300\n",
    "hidden_units = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    enc_embedding_layer = Embedding(len(x_word_index) + 1,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[encoder_emb],\n",
    "                                input_length=MAX_TEXT_LENGTH,\n",
    "                                trainable=False,\n",
    "                                name='EncoderEmbeddingLayer')\n",
    "\n",
    "\n",
    "    dec_embedding_layer = Embedding(len(y_word_index) + 1,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[decoder_emb],\n",
    "                                input_length=MAX_HEADLINE_LENGTH,\n",
    "                                trainable=False,\n",
    "                                name='DecoderEmbeddingLayer')\n",
    "\n",
    "\n",
    "    # Encoder \n",
    "\n",
    "    # Encoder input \n",
    "    # 2D (sequence_length, None), where sequence length is the MAX_LEN unified by padding in preprocessing\n",
    "    encoder_inputs = Input(shape=(MAX_TEXT_LENGTH,), name=\"EncoderInput\") \n",
    "    enc_emb = enc_embedding_layer(encoder_inputs) \n",
    "\n",
    "\n",
    "    #LSTM 1 \n",
    "    encoder_lstm1 = LSTM(hidden_units,return_sequences=True,return_state=True,go_backwards=True, name='EncLSTM1') \n",
    "    encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb) \n",
    "\n",
    "    #LSTM 2 \n",
    "    encoder_lstm2 = LSTM(hidden_units,return_sequences=True,return_state=True,go_backwards=True, name='EncLSTM2') \n",
    "    encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1) \n",
    "\n",
    "    #LSTM 3 \n",
    "    encoder_lstm3=LSTM(hidden_units, return_state=True, return_sequences=True,go_backwards=True, name='EncLSTM3') \n",
    "    encoder_output3, state_h3, state_c3= encoder_lstm3(encoder_output2) \n",
    "\n",
    "    #LSTM 4 \n",
    "    encoder_lstm4=LSTM(hidden_units, return_state=True, return_sequences=True,go_backwards=True, name='EncLSTM4') \n",
    "    encoder_outputs, state_h, state_c= encoder_lstm4(encoder_output3) \n",
    "\n",
    "    # Decoder \n",
    "\n",
    "    decoder_inputs = Input(shape=(None,), name = 'DecoderInput') \n",
    "    #dec_emb_layer = Embedding(y_voc_size, latent_dim,trainable=True) \n",
    "    dec_emb = dec_embedding_layer(decoder_inputs) \n",
    "\n",
    "    #LSTM using encoder_states as initial state\n",
    "    decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True, name='DecLSTM1') \n",
    "    decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c]) \n",
    "\n",
    "    #Attention Layer\n",
    "    attn_layer = AttentionLayer(name='attention_layer') \n",
    "    attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs]) \n",
    "\n",
    "    # Concat attention output and decoder LSTM output \n",
    "    decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "    #Dense layer\n",
    "    decoder_dense = TimeDistributed(Dense(len(y_word_index)+1, activation='softmax')) \n",
    "    decoder_outputs = decoder_dense(decoder_concat_input) \n",
    "\n",
    "    return Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "EncoderInput (InputLayer)       [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "EncoderEmbeddingLayer (Embeddin (None, 100, 300)     63844200    EncoderInput[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "EncLSTM1 (LSTM)                 [(None, 100, 300), ( 721200      EncoderEmbeddingLayer[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "EncLSTM2 (LSTM)                 [(None, 100, 300), ( 721200      EncLSTM1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "DecoderInput (InputLayer)       [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "EncLSTM3 (LSTM)                 [(None, 100, 300), ( 721200      EncLSTM2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "DecoderEmbeddingLayer (Embeddin (None, None, 300)    23604900    DecoderInput[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "EncLSTM4 (LSTM)                 [(None, 100, 300), ( 721200      EncLSTM3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "DecLSTM1 (LSTM)                 [(None, None, 300),  721200      DecoderEmbeddingLayer[0][0]      \n",
      "                                                                 EncLSTM4[0][1]                   \n",
      "                                                                 EncLSTM4[0][2]                   \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer ((None, None, 300),  180300      EncLSTM4[0][0]                   \n",
      "                                                                 DecLSTM1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, None, 600)    0           DecLSTM1[0][0]                   \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, None, 78683)  47288483    concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 138,523,883\n",
      "Trainable params: 51,074,783\n",
      "Non-trainable params: 87,449,100\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "with strategy.scope():\n",
    "    parallel_model = create_model() \n",
    "    parallel_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "parallel_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "parallel_model.load_weights(\"model_fd_111519_300units_punct.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = parallel_model.input[0]\n",
    "encoder_model = Model(inputs = encoder_inputs, outputs = parallel_model.get_layer('EncLSTM4').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_state_input_h = Input(shape=(hidden_units,))\n",
    "decoder_state_input_c = Input(shape=(hidden_units,))\n",
    "decoder_hidden_state_input = Input(shape=(MAX_TEXT_LENGTH,hidden_units))\n",
    "\n",
    "# Get the embeddings of the decoder sequence\n",
    "dec_emb2= parallel_model.get_layer('DecoderEmbeddingLayer')(parallel_model.input[1])\n",
    "\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = parallel_model.get_layer('DecLSTM1')(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "#attention inference\n",
    "attn_out_inf, attn_states_inf = parallel_model.get_layer('attention_layer')([decoder_hidden_state_input, decoder_outputs2])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "decoder_outputs2 = parallel_model.layers[-1](decoder_inf_concat) #This is the time distributed layer\n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "[parallel_model.input[1]] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "[decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2summary(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0 ):\n",
    "            newString=newString+y_index_word[i]+' '\n",
    "    return newString\n",
    "\n",
    "def seq2text(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            newString=newString+x_index_word[i]+' '\n",
    "    return newString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_beam_search_sentences(input_seq, beam=3):\n",
    "    # Encode the input as state vectors.\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "\n",
    "    top_sentences = {}\n",
    "    \n",
    "    def top_tokens(last_token, out, h, c):\n",
    "        output_tokens, h_new, c_new = decoder_model.predict([[last_token]] + [out, h, c])\n",
    "        top_token_indexes = np.argsort(output_tokens[0, -1, :])[-beam:]\n",
    "        top_probabilities = output_tokens[0,-1, top_token_indexes]\n",
    "        return top_token_indexes, top_probabilities, h_new, c_new\n",
    "        \n",
    "    #first set of tokens when feeding encoder states and 0 as the first token to the decoder.\n",
    "    first_tokens, first_probabilities, h, c = top_tokens(0, e_out, e_h, e_c)\n",
    "    for first_token, first_probability in zip(first_tokens, first_probabilities):\n",
    "        #initialize top sentences, their corresponding probabilities and states\n",
    "        top_sentences[y_index_word.get(first_token, '')] = (first_probability, h, c)\n",
    "    \n",
    "    \n",
    "    #loop to iterate over next tokens\n",
    "    len = 1\n",
    "    while len < MAX_HEADLINE_LENGTH:\n",
    "        candidate_sentences = {}\n",
    "        for sentence, (probability, h, c) in top_sentences.items():\n",
    "            last_word = sentence.split()[-1] #pick the last word in the sentence as next word\n",
    "            if(last_word != '.'):\n",
    "                token = y_word_index.get(last_word, 0) \n",
    "                next_tokens, next_probabilities, h_next, c_next = top_tokens(token, e_out, h, c)\n",
    "                for next_token, next_probability in zip(next_tokens, next_probabilities):\n",
    "                    new_sentence = sentence.strip() + ' ' + y_index_word.get(next_token, '')\n",
    "                    candidate_sentences[new_sentence.strip()] = (probability * next_probability, h_next, c_next)\n",
    "            else:\n",
    "                candidate_sentences[sentence] = (probability, h, c)\n",
    "\n",
    "        #print('Candidate sentences')\n",
    "        #print(candidate_sentences.keys())\n",
    "        \n",
    "        #remove low probability candidates\n",
    "        low_probability_candidates = sorted(candidate_sentences, key=lambda k: candidate_sentences.get(k)[0])[:-beam]\n",
    "        for low_probability_candidate in low_probability_candidates:\n",
    "            candidate_sentences.pop(low_probability_candidate)\n",
    "        \n",
    "        #Now all candidates left have highest probabilities.\n",
    "        top_sentences = candidate_sentences\n",
    "        len = len + 1\n",
    "        #print('Sentences at the bottom of the loop')\n",
    "        #print(top_sentences.keys())\n",
    "        \n",
    "\n",
    "    return top_sentences\n",
    "\n",
    "def decode_sequence(input_seq, beam=3):\n",
    "    top_sentences_obj = get_top_beam_search_sentences(input_seq.reshape(1,-1), beam)\n",
    "    l = [(sen, prob) for sen, (prob, _, _) in top_sentences_obj.items()]\n",
    "    return sorted(l, key = lambda x:-x[1])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import rouge_n_sentence_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_decode(x): \n",
    "    gen_output = decode_sequence(x.reshape(1,-1), 1).split()\n",
    "    candidate = [item for item in gen_output if (item!=\".\" and item!=\"\")] \n",
    "    return(candidate)\n",
    "\n",
    "def return_summary(x): \n",
    "    gen_summary = seq2summary(x).split()\n",
    "    reference =  [item for item in gen_summary if (item!=\".\" and item!=\"\")] \n",
    "    return(reference)\n",
    "\n",
    "# Other useful links to keep in mind: \n",
    "# https://stackoverflow.com/questions/38045290/text-summarization-evaluation-bleu-vs-rouge\n",
    "\n",
    "def calc_indiv_rouge(id_text, text_df, headline_df, rouge_n): \n",
    "    # This function will take the following as inputs: \n",
    "    # id_text: the index you are interested in \n",
    "    # gen_text_df: the sequences that hold the full text \n",
    "    # headline_df: the sequences that hold the headline text \n",
    "    \n",
    "    # -- Step 1: generate the decoded sequence from a given sample of text\n",
    "    gen_output = decode_sequence(text_df[id_text].reshape(1,-1))\n",
    "    split_output = gen_output.split(\" \")\n",
    "    candidate = [item for item in split_output if (item!=\".\" and item!=\"\")] #get rid of empty spaces and periods \n",
    "    # -- Step 2: generate the true headline summary from our labelled headline text\n",
    "    gen_ref = seq2summary(headline_df[id_text])\n",
    "    split_ref = gen_ref.split(\" \")\n",
    "    #get rid of empty spaces and periods (there shouldn't be any as we already cleaned the headline, but just in case)\n",
    "    reference = [item for item in split_ref if (item!=\".\" and item!=\"\")] \n",
    "    # -- Step 3: calculate rouge\n",
    "    recall, precision, rouge = rouge_n_sentence_level(candidate, reference, rouge_n)\n",
    "    # rouge is actually an f-score of the recall and precision \n",
    "    return(recall, precision, rouge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************"
     ]
    }
   ],
   "source": [
    "chunkholder = list(chunks(range(0, 10000), 100))\n",
    "\n",
    "result = []\n",
    "cumulative_list=list(range(100,10100,100))\n",
    "cumulative_avg = []\n",
    "for chunk in chunkholder: \n",
    "#    -- change the function return_decode to whatever function you need (i.e. beam search)\n",
    "   decode_machine_summary = [return_decode(item) for item in x_test[chunk]]\n",
    "#    -- for beamsearch, I used: decode_machine_summary = [beam_decode_sequence(item).split(' ') for item in x_test[chunk]]\n",
    "   decode_ref_summary = [return_summary(item) for item in y_test[chunk]]\n",
    "   testset = pd.concat([pd.Series(decode_machine_summary), pd.Series(decode_ref_summary)], axis=1)\n",
    "   testset.columns = ['gen_summary','ref_summary']\n",
    "   testset['rouge_cols']=testset.apply(lambda x: rouge_n_sentence_level(x['gen_summary'], x['ref_summary'], 1), axis=1)\n",
    "   allrouge = testset['rouge_cols'].apply(pd.Series)\n",
    "   allrouge.columns = ['recall','precision','f1score']\n",
    "   final = pd.concat([testset,allrouge],axis=1) \n",
    "   result.append(final)\n",
    "   print('*', end = '')\n",
    "\n",
    "\n",
    "result_df = pd.concat(result).reset_index(drop=True) #this holds the decoded sequences up to 10,000\n",
    "\n",
    "\n",
    "\n",
    "for i in cumulative_list: \n",
    "#   -- calculate the average from every 100 chunk (i.e. 0 to 100, 0 to 200, 0 to 300, etc)\n",
    "   temp = result_df.loc[0:i,'recall':'f1score'].apply(lambda x: np.nanmean(x), axis=0)\n",
    "   cumulative_avg.append({'recall': temp['recall'],'precision': temp['precision'],'f1score': temp['f1score']})\n",
    "\n",
    "cumulative_avg_df = pd.DataFrame(cumulative_avg) # This holds the cumulative sample averages up to 10,000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.183909</td>\n",
       "      <td>0.285852</td>\n",
       "      <td>0.206293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.168883</td>\n",
       "      <td>0.258097</td>\n",
       "      <td>0.188075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.163711</td>\n",
       "      <td>0.254809</td>\n",
       "      <td>0.185136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.163952</td>\n",
       "      <td>0.253599</td>\n",
       "      <td>0.184706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.163115</td>\n",
       "      <td>0.250067</td>\n",
       "      <td>0.183935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.160016</td>\n",
       "      <td>0.251615</td>\n",
       "      <td>0.181419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.160250</td>\n",
       "      <td>0.252079</td>\n",
       "      <td>0.181726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.160404</td>\n",
       "      <td>0.252579</td>\n",
       "      <td>0.181964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.160366</td>\n",
       "      <td>0.252664</td>\n",
       "      <td>0.181913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.160312</td>\n",
       "      <td>0.252536</td>\n",
       "      <td>0.181850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      recall  precision   f1score\n",
       "0   0.183909   0.285852  0.206293\n",
       "1   0.168883   0.258097  0.188075\n",
       "2   0.163711   0.254809  0.185136\n",
       "3   0.163952   0.253599  0.184706\n",
       "4   0.163115   0.250067  0.183935\n",
       "..       ...        ...       ...\n",
       "95  0.160016   0.251615  0.181419\n",
       "96  0.160250   0.252079  0.181726\n",
       "97  0.160404   0.252579  0.181964\n",
       "98  0.160366   0.252664  0.181913\n",
       "99  0.160312   0.252536  0.181850\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#beam\n",
    "cumulative_avg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.197287</td>\n",
       "      <td>0.256464</td>\n",
       "      <td>0.202869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.187534</td>\n",
       "      <td>0.241549</td>\n",
       "      <td>0.191705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.184545</td>\n",
       "      <td>0.239680</td>\n",
       "      <td>0.191062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.184358</td>\n",
       "      <td>0.243247</td>\n",
       "      <td>0.191215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.186328</td>\n",
       "      <td>0.240658</td>\n",
       "      <td>0.192652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.179907</td>\n",
       "      <td>0.241002</td>\n",
       "      <td>0.189367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.180015</td>\n",
       "      <td>0.241442</td>\n",
       "      <td>0.189596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.180171</td>\n",
       "      <td>0.241871</td>\n",
       "      <td>0.189820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.180200</td>\n",
       "      <td>0.242097</td>\n",
       "      <td>0.189895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.180064</td>\n",
       "      <td>0.241928</td>\n",
       "      <td>0.189776</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      recall  precision   f1score\n",
       "0   0.197287   0.256464  0.202869\n",
       "1   0.187534   0.241549  0.191705\n",
       "2   0.184545   0.239680  0.191062\n",
       "3   0.184358   0.243247  0.191215\n",
       "4   0.186328   0.240658  0.192652\n",
       "..       ...        ...       ...\n",
       "95  0.179907   0.241002  0.189367\n",
       "96  0.180015   0.241442  0.189596\n",
       "97  0.180171   0.241871  0.189820\n",
       "98  0.180200   0.242097  0.189895\n",
       "99  0.180064   0.241928  0.189776\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#greedy\n",
    "cumulative_avg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------Beam Search-------------------------- 3456\n",
      "[('eat fermented foods .', 0.006856282), ('eat foods that contain probiotics .', 0.0051042293), ('eat foods containing probiotics .', 0.0050948975), ('eat foods rich in fiber .', 0.0041178283), ('eat foods rich in potassium .', 0.0030482467)]\n",
      "------------Greedy Search------------------------\n",
      "eat foods that contain probiotics .\n",
      "------------Ground Truth-------------------------\n",
      "eat probiotics and prebiotics . \n",
      "------------Document-----------------------------\n",
      "they can also protect your gut from harmful bacteria that irritates your bowels . since it is hard to gauge how many colony forming units of probiotics cfus are in foods , eat a variety of foods known to contain probiotics and prebiotics . to get probiotics in your diet , eat leafy green vegetables kale , spinach , swiss chard , spinach , beet greens , collard greens , mustard greens , broccoli , cauliflower , and cabbage . to get prebiotics , eat chicory root jerusalem artichoke dandelion greens garlic leeks asparagus wheat bran baked wheat flour bananas \n",
      "=================================================\n"
     ]
    }
   ],
   "source": [
    "for i in range (3456, 3457):\n",
    "    res = get_top_beam_search_sentences(x_test[i].reshape(1,-1), 5)\n",
    "    l = [(sen, prob) for sen, (prob, _, _) in res.items()]\n",
    "    print('------------Beam Search--------------------------', i)\n",
    "    print(sorted(l, key = lambda x:-x[1]))\n",
    "    print('------------Greedy Search------------------------')\n",
    "    print(decode_sequence(x_test[i].reshape(1,-1)))\n",
    "    print('------------Ground Truth-------------------------')\n",
    "    print(seq2summary(y_test[i]))\n",
    "    print('------------Document-----------------------------')    \n",
    "    print(seq2text(x_test[i]))\n",
    "    print('=================================================')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
