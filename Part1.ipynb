{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from w266_common import utils, vocabulary, glove_helper\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras as k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs_to_tokens(docs):\n",
    "    \"\"\"Returns an flattened list of the words in the sentences, with padding for a trigram model.\"\"\"\n",
    "    padded_sentences = [[u\"<s>\", u\"<s>\"] + d + [u\"</s>\"] for d in docs]\n",
    "    # This will canonicalize words, and replace anything not in vocab with <unk>\n",
    "    return padded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pd.read_csv('wikihow/wikihowAll.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_words = f.applymap(lambda text: k.preprocessing.text.text_to_word_sequence(str(text), filters='#$%&()*+-/:<=>@[\\\\]^_`{|}~\\t\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc, test_doc, train_sum, test_sum = train_test_split(f_words['text'], f_words['headline'], test_size=0.2, random_state=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vocabulary.Vocabulary((utils.canonicalize_word(w) for w in utils.flatten(train_doc)), size=V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc_tokens = docs_to_tokens(train_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc_tokens = docs_to_tokens(test_doc)\n",
    "train_sum_tokens = docs_to_tokens(train_sum)\n",
    "test_sum_tokens = docs_to_tokens(test_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data: \n",
      " ['<s>', '<s>', 'ground', 'venison', 'works', 'well', 'for', 'burgers,', 'meat', 'loaves,', 'and', 'as', 'a', 'general', 'alternative', 'to', 'any', 'ground', 'beef', 'recipe,', 'but', \"it's\", 'perfectly', 'suited', 'for', 'chili.', 'whether', 'you', 'want', 'to', 'make', 'straight', 'up', 'venison', 'chili,', 'or', 'combine', 'venison', 'with', 'a', 'small', 'amount', 'of', 'stew', 'beef', 'or', 'pork', 'sausage,', \"it's\", 'a', 'great', 'base', 'for', 'a', 'hearty', 'chili.', 'a', 'pound', 'should', 'make', '8', '12', 'servings.', '\"chili', 'meat\"', 'refers', 'to', 'a', 'specific', 'coarseness', 'of', 'grind,', 'usually', 'somewhat', 'finer', 'than', '\"ground\"', 'venison.', 'if', 'you', 'want', 'a', 'finer', 'grind,', 'have', 'your', 'deer', 'processor', 'grind', 'up', 'some', 'chili', 'meat,', 'or', 'purchase', 'your', 'own', 'meat', 'grinder', 'to', 'grind', 'up', 'your', 'own.if', 'you', 'like', 'a', 'more', 'texas', 'style', 'chili,', 'stew', 'meat', 'chunks', 'would', 'be', 'more', 'appropriate,', 'and', \"you'll\", 'probably', 'want', 'to', 'cook', 'it', 'at', 'a', 'lower', 'temperature', 'for', 'a', 'longer', 'period', 'of', 'time,', 'though', 'the', 'ingredients', 'and', 'the', 'technique', 'will', 'be', 'basically', 'the', 'same.', ',', 'add', 'one', 'or', 'two', 'tablespoons', 'of', 'cooking', 'oil', 'to', 'the', 'bottom', 'of', 'a', 'heavy', 'bottomed', 'stew', 'pot,', 'and', 'add', 'your', 'ground', 'venison.', 'using', 'a', 'wooden', 'spoon,', 'stir', 'the', 'meat', 'around', 'as', 'it', 'browns', 'up.', 'just', 'before', 'it', 'turns', 'dark', 'all', 'over,', 'add', 'one', 'medium', 'chopped', 'yellow', 'onion,', 'a', 'diced', 'red', 'pepper,', 'and', 'three', 'or', 'four', 'cloves', 'of', 'minced', 'garlic', 'to', 'the', 'ground', 'venison.,', 'as', 'the', 'onions', 'start', 'to', 'brown,', \"it's\", 'time', 'to', 'add', 'the', 'beans', 'and', 'the', 'tomatoes.', 'use', 'a', 'can', 'of', 'drained', 'red', 'kidney', 'beans,', 'or', 'a', 'mixture', 'of', 'red', 'beans,', 'navy', 'beans,', 'and', 'garbanzo', 'beans,', 'if', 'you', 'like.', 'about', '12', 'ounces', 'should', 'be', 'perfect.', 'use', 'an', '18', 'oz.', 'can', 'of', 'crushed', 'tomatoes,', 'plus', 'a', 'tablespoon', 'of', 'tomato', 'paste', 'to', 'provide', 'the', 'bass', 'for', 'the', 'chili.', 'if', 'you', 'want', 'to', 'use', 'fresh', 'tomatoes,', 'start', 'with', 'about', 'four', 'ripe', 'tomatoes,', 'chopping', 'them', 'roughly', 'and', 'saving', 'all', 'the', 'juices.', 'keep', 'a', 'close', 'eye', 'and', 'add', 'a', 'little', 'water', 'if', 'the', 'chili', 'needs', 'more', 'moisture', 'added.', 'if', 'beans', \"aren't\", 'your', 'thing,', 'follow', 'the', 'chili', 'recipe', 'you', 'like', 'to', 'make.', 'venison', 'is', 'perfectly', 'amenable', 'to', 'most', 'green', 'chili', 'recipes,', 'or', 'other', 'types', 'of', 'regional', 'chili', 'you', 'might', 'prefer.', 'use', 'the', 'flavors', 'and', 'seasonings', 'that', 'you', 'like', 'and', 'see', 'if', 'you', 'like', 'it', 'better', 'with', 'venison.', ',', 'season', 'the', 'chili', 'to', 'your', 'taste.', 'if', 'you', 'like', 'it', 'very', 'strong,', 'you', 'can', 'add', 'more,', 'or', 'stronger', 'chili', 'powder,', 'alongside', 'a', 'teaspoon', 'of', 'cumin,', 'cayenne,', 'and', 'any', 'other', 'spices', 'you', 'like', 'in', 'your', 'chili.', 'if', 'you', \"don't\", 'like', 'chili', 'strong,', 'add', 'thyme,', 'cumin,', 'some', 'coriander', 'powder,', 'and', 'other', 'aromatics.', 'add', 'salt', 'and', 'pepper', 'to', 'taste.', 'to', 'get', 'at', 'that', 'distinctive', 'chili', 'taste,', \"you'll\", 'need', 'at', 'least', 'a', 'bit', 'of', 'chili', 'powder.', 'add', 'a', 'teaspoon', 'at', 'a', 'time.', 'you', 'can', 'always', 'add', 'more', 'later.', ',', 'turn', 'the', 'heat', 'to', 'low,', 'pop', 'the', 'lid', 'on,', 'and', 'let', 'the', 'chili', 'simmer', 'gently', 'for', 'a', 'couple', 'hours.', 'the', 'meat', 'should', 'cook', 'in', 'about', '30', 'minutes', 'or', 'so,', 'but', 'the', 'flavors', 'will', 'really', 'come', 'together', 'with', 'at', 'least', 'an', 'hour', 'or', 'two', 'of', 'slow', 'cooking.', 'taste', 'it', 'after', 'thirty', 'minutes', 'to', 'adjust', 'the', 'seasoning', 'and', 'add', 'more', 'chili', 'powder,', 'if', 'necessary.', 'serve', 'with', 'cornbread.if', 'you', 'prefer,', 'you', 'can', 'also', 'transfer', 'the', 'chili', 'to', 'a', 'slow', 'cooker', 'and', 'let', 'it', 'sit', 'all', 'day,', 'or', 'overnight', 'to', 'really', 'let', 'the', 'flavors', 'blend.', 'in', 'general,', 'the', 'longer', 'it', 'cooks,', 'the', 'better', \"it'll\", 'be.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample data: \\n\", repr(train_doc_tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading GloVe vectors to data/glove\n",
      "Loading vectors from data/glove/glove.6B.zip\n",
      "Parsing file: data/glove/glove.6B.zip:glove.6B.100d.txt\n",
      "Found 400,000 words.\n",
      "Parsing vectors... Done! (W.shape = (400003, 100))\n"
     ]
    }
   ],
   "source": [
    "hands = glove_helper.Hands(ndim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172292"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_doc_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = max(map(len, train_doc_tokens + test_doc_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1026 12:20:32.165593 4565179712 deprecation.py:506] From /usr/local/lib/python3.7/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "encoder_embedding_layer = k.layers.Embedding(input_dim = hands.W.shape[0], \n",
    "                                    output_dim = hands.W.shape[1],\n",
    "                                    input_length = MAX_LEN,\n",
    "                                    weights = hands.W,\n",
    "                                    trainable = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_embedding_layer = k.layers.Embedding(input_dim = hands.W.shape[0], \n",
    "                                    output_dim = hands.W.shape[1],\n",
    "                                    input_length = MAX_LEN,\n",
    "                                    weights = hands.W,\n",
    "                                    trainable = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
